# NLP
1. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same context-free representation in “bank account” and “bank of the river.”
2. Contextual models instead generate a representation of each word that is based on the other words in the sentence. Contextual representations can further be unidirectional or bidirectional. For example, in the sentence “I accessed the bank account,” a unidirectional contextual model would represent “bank” based on “I accessed the” but not “account.” However, BERT represents “bank” using both its previous and next context — “I accessed the … account” — starting from the very bottom of a deep neural network, making it deeply bidirectional.

# Traditional Embedding
## BoW
Each word or n-gram is linked to a vector index and marked as 0 or 1 depending on whether it occurs in a given document.
- don’t encode any information with regards to the meaning of a given word.
- word occurrences are evenly weighted independently of how frequently or what context they occur. 

## TF-IDF
Numerical statistic that is intended to reflect how important a word or n-gram is to a document in a collection or corpus. 
- still unable to capture the word meaning.

## Distributional Embeddings
Enable word vectors to encapsulate contextual context.

# Neural Embeddings
Then the field quickly realized it’s a great idea to use embeddings that were pre-trained on vast amounts of text data instead of training them alongside the model on what was frequently a small dataset. So it became possible to download a list of words and their embeddings generated by pre-training with Word2Vec or GloVe.
## Word2vec(Doc2vec)
Word2vec is a neural network structure to generate word embedding by training the model on a supervised classification problem. In the word2vec architecture, the two algorithm names are “continuous bag of words” (CBOW) and “skip-gram” (SG). Word2vec only takes local contexts into account and does not take advantage of global context. 

Word2Vec showed that we can use a vector (a list of numbers) to properly represent words in a way that captures semantic or meaning-related relationships (e.g. the ability to tell if words are similar, or opposites, or that a pair of words like “Stockholm” and “Sweden” have the same relationship between them as “Cairo” and “Egypt” have between them) as well as syntactic, or grammar-based, relationships (e.g. the relationship between “had” and “has” is the same as that between “was” and “is”).

Word2vec is a predictive model which learns their vectors in order to improve their predictive ability of Loss(target word | context words; Vectors), i.e. the loss of predicting the target words from the context words given the vector representations. In word2vec, this is cast as a feed-forward neural network and optimized as such using SGD, etc.

![model overview](/pictures/word2vec.png)

1. Example: https://towardsdatascience.com/word2vec-from-scratch-with-numpy-8786ddd49e72

## GloVe
GloVe is a count-based model which learns their vectors by doing dimensionality reduction on the co-occurrence information. Firsly, construct a large matrix of (word * context) co-occurrence information. Secondly, factorize this matrix to a lower-dimensional (word * features) matrix, where each row yield a vector representation for each word. In the specific case of GloVe, the counts matrix is preprocessed by normalizing the counts and log-smoothing them.

This is an example of the GloVe embedding of the word “stick” (with an embedding vector size of 200)
![embedding example](/Knowledge_Based/pictures/GloVe_embedding_example.png)

1. The difference between GloVe and Word2vec: https://www.quora.com/How-is-GloVe-different-from-word2vec

## Pre-ELMo(Tag LM)
-  Step1: pretrain word embeddings and language model
-  Step2: prepare word embedding and LM embedding for each token in the input sequence
-  Step3: use both word embeddings and LM embeddings in the sequence tagging model

## ELMo
![model overview](/Knowledge_Based/pictures/ELMo.gif)
-  Unlike traditional word embeddings such as word2vec and GLoVe, the ELMo vector assigned to a token or word is actually a function of the entire sentence containing that word. Therefore, the same word can have different word vectors under different contexts.
-  ELMo gained its language understanding from being trained to predict the next word in a sequence of words - a task called Language Modeling. 
-  ELMo's secrete is to train a bi-directional LSTM and concatenate hidden layers followed by weighted summation.

1. Simple introduction: https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/
2. Nice explanation: http://jalammar.github.io/illustrated-bert/

# Transformer(go beyond LSTM)
While ELMo was able to overcome many of the shortcomings of previous word embeddings architectures, it difficult to learn longer sequences of text such as sentences or paragraphs and also makes it slower to train. However, the transformer architecture allows inputs to be processed simultaneously. It was mentioned in a paper [Attention is all you need](https://arxiv.org/abs/1706.03762).

A high-level look:
![high level look](/Knowledge_Based/pictures/transformer_high_level_look.png)
- Encoders: the encoding component is a stack of encoders, all identical in structure.
- Decoders: the decoding component is a stack of decoders of the same number, has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence.
![high level look](/Knowledge_Based/pictures/transformer_encoder_decoder.png)

A Detailed look:
![high level look](/Knowledge_Based/pictures/transformer_detailed_encoder_decoder.png)
- Encoders: 
    - turn each input word into a vector using an embedding algorithm
    - add positional encoding, which helps it determine the position of each word, or the distance between different words in the sequence
    - self attention is the method the Transformer uses to bake the “understanding” of other relevant words into the one we’re currently processing
    - layer normalization: reduce the training time, very effective at stabilizing the hidden state dynamics in recurrent networks
    - output of the top encoder is then transformed into a set of attention vectors K and V, which are used by each decoder in its “encoder-decoder attention” layer, helping the decoder focus on appropriate places in the input sequence
- Decoders:
    - the self-attention layer is only allowed to attend to earlier positions in the output sequence, which is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation
    - the “Encoder-Decoder Attention” layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack
![decoder look](/Knowledge_Based/pictures/transformer_decoder.gif)
- Linear and Softmax Layer

![decoder look](/Knowledge_Based/pictures/transformer_decoder_output_softmax.png)

1. Illustrated Transformer: http://jalammar.github.io/illustrated-transformer/

## BERT
Inportant: They employed masked language modeling. In other words, they hid 15% of the words and used their position information to infer them. Finally, they also mixed it up a little bit to make the learning process more effective.

InputExample Format(tsv): 
-  guid: Unique ID for the row
-  text_a: Text
-  text_b: A column of the same letter for all rows. BERT wants, but we don’t use
-  labels: The text for row (will be empty for test data)

InputFeature Requires:
-  Require: convert InputExample to InputFeature (purely numberical data)
-  Steps: 
    -  tokenizing embedding: tokenize the text, truncate the long sequence or pad the short sequence to the given sequence length (max 512, 128 ...)
    -  sentence embedding
    -  transformer positional embedding
    
InputFeature Format:
- input_ids: list of numberical ids for the tokenised text
- input_mask: will be set to 1 for real tokens and 0 for the padding tokens
- segment_ids: sigle sentence or multiple sentence
- label_ids: one-hot encoded labels for the text

Masked LM
- uncased BERT Base: 12 layers encoders, which embedding do you want, probably the sum of last 4 layer

Output
- "pooled_output" for classification tasks on an entire sentence
- "sequence_outputs" for token-level output

Estimate
- classification matrix

Next Sentence Prediction 

Difference

![model overview](/Knowledge_Based/pictures/BERT_ELMo.png)

1. https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270
2. https://medium.com/swlh/a-simple-guide-on-using-bert-for-text-classification-bbf041ac8d04
3. https://towardsdatascience.com/nlp-extract-contextualized-word-embeddings-from-bert-keras-tf-67ef29f60a7b
4. [movie setiment analysis](https://github.com/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb)

## Comparison among NLP models
![model overview](/Knowledge_Based/pictures/comparison_among_NLP_model.png)

# Neural NLP Architectures
## MNL

## CNN
CNN works as a n-gram feature extractors for embeddings. 
- convolutional layer(kernel/filter) is to extract the high-level features.
- pooling layer (max, average pooling) is to decrease the computational power required to process the data through dimensionality reduction. 
- fully connected layer with dropout and softmax output.

1. Simple explaination: https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53

## RNN(LSTMs and GRUs)

# Machine Translation
# Seq2Seq
# Attention Mechanism
Attention is a concept that helped improve the performance of neural machine translation applications.

Core Idea: on each step of the decoder, use direact connection to encoder to focus on a particular part of the source sentense.

Problem to Solve: for example in NMT(Neural Machine Translation), they work by encoding the sentence into a vector using RNN and then decoding them based on that vector also using RNN, which is an information bottleneck for 2 reasons:
-  hard to capture all the information in one vector
-  hard to produce a good translation based on that vector(as we all know RNN has problem to deal with long-range dependencies)

Great: 
-  improve NMT by allowing decoder to focus on certainparts of the source
-  solve the bottleneck problem by allowing decoder to look directly at source
-  helps with vanishing gradient by providing shortcut to faraway states
-  provides interpretability by inspecting attention distribution, so we get soft alignment for free (it's cool because we never explicitly trained an alignment system and the network just learned alignment by itself)

Cost of Attention: we need to calculate an attention value for each combination of input and output word (you have a 50-word input sequence and generate a 50-word output sequence that would be 2500 attention values) but it hasn’t stopped attention mechanisms from becoming quite popular and performing well on many tasks.

An alternative approach to attention is to use Reinforcement Learning to predict an approximate location to focus to. 

Different Attentions
-  generalized attention
-  self attention
-  multi-Head Attention

1. Nice explaination: http://www.wildml.com/2016/01/attention-and-memory-in-deep-learning-and-nlp/
2. CS224N: https://web.stanford.edu/class/cs224n/slides/cs224n-2019-lecture13-contextual-representations.pdf
3. 动画解释：https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/

## Evaluation: BLEU(Bilingual Evaluation Understudy)

